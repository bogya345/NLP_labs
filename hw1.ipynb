{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEEKQwtYDMUz"
      },
      "source": [
        "**SOFT DEADLINE:** `20.03.2022 23:59 msk` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-IAo9sYDMU-"
      },
      "source": [
        "# [5 points] Part 1. Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SNyjuGjDMU_"
      },
      "source": [
        "The task is to clear the text data of the crawled web-pages from different sites. \n",
        "\n",
        "It is necessary to ensure that the distribution of the 100 most frequent words includes only meaningful words in english language (not particles, conjunctions, prepositions, numbers, tags, symbols)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA4VkGl2DMVB"
      },
      "source": [
        "Determine the order of operations below and carry out the appropriate cleaning.\n",
        "\n",
        "1. Remove non-english words\n",
        "1. Remove html-tags (try to do it with regular expression, or play with beautifulsoap library)\n",
        "1. Apply lemmatization / stemming\n",
        "1. Remove stop-words\n",
        "1. Additional processing - At your own initiative, if this helps to obtain a better distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The choosen order:\n",
        "1. Remove html-tags\n",
        "2. Remove non-english words\n",
        "3. Remove stop-words\n",
        "4. Apply lemmatization / stemming\n",
        "5. Additional processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjLsB9pdDMVC"
      },
      "source": [
        "#### Hints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk6ZLncvDMVD"
      },
      "source": [
        "1. To do text processing you may use nltk and re libraries\n",
        "1. and / or any other libraries on your choise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCbTa1OiDMVE"
      },
      "source": [
        "#### Data reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXbG649IDMVG"
      },
      "source": [
        "The dataset for this part can be downloaded here: `https://drive.google.com/file/d/1wLwo83J-ikCCZY2RAoYx8NghaSaQ-lBA/view?usp=sharing`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# path = './storage/hw1/train.csv'\n",
        "path = './storage/hw1/web_sites_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "HN8UxSDkDMVH"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;html&gt;\\n&lt;head profile=\"http://www.w3.org/2005/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;html&gt;\\n&lt;head profile=\"http://www.w3.org/2005/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;html&gt;\\n&lt;head profile=\"http://www.w3.org/2005/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;html&gt;\\n&lt;head profile=\"http://www.w3.org/2005/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;html&gt;\\n&lt;head profile=\"http://www.w3.org/2005/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>&lt;html&gt;\\n&lt;head profile=\"http://www.w3.org/2005/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>&lt;html&gt;\\n&lt;head profile=\"http://www.w3.org/2005/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>&lt;html&gt;\\n&lt;head profile=\"http://www.w3.org/2005/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>&lt;html&gt;\\n&lt;head profile=\"http://www.w3.org/2005/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>&lt;html&gt;\\n&lt;head profile=\"http://www.w3.org/2005/...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text\n",
              "0   <html>\\n<head profile=\"http://www.w3.org/2005/...\n",
              "1   <html>\\n<head profile=\"http://www.w3.org/2005/...\n",
              "2   <html>\\n<head profile=\"http://www.w3.org/2005/...\n",
              "3   <html>\\n<head profile=\"http://www.w3.org/2005/...\n",
              "4   <html>\\n<head profile=\"http://www.w3.org/2005/...\n",
              "..                                                ...\n",
              "95  <html>\\n<head profile=\"http://www.w3.org/2005/...\n",
              "96  <html>\\n<head profile=\"http://www.w3.org/2005/...\n",
              "97  <html>\\n<head profile=\"http://www.w3.org/2005/...\n",
              "98  <html>\\n<head profile=\"http://www.w3.org/2005/...\n",
              "99  <html>\\n<head profile=\"http://www.w3.org/2005/...\n",
              "\n",
              "[100 rows x 1 columns]"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "limit = 100\n",
        "\n",
        "data = pd.read_csv(path, nrows=limit)\n",
        "\n",
        "# for debug\n",
        "# data = data.sample(100)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<html>\\n<head profile=\"http://www.w3.org/2005/10/profile\">\\n<LINK REL=\"SHORTCUT ICON\" href=\"http://i.bookmooch.com/favicon.ico\"> \\n<link rel=\"icon\" type=\"image/png\" href=\"http://i.bookmooch.com/favicon.p',\n",
              " '<html>\\n<head profile=\"http://www.w3.org/2005/10/profile\">\\n<LINK REL=\"SHORTCUT ICON\" href=\"http://i.bookmooch.com/favicon.ico\"> \\n<link rel=\"icon\" type=\"image/png\" href=\"http://i.bookmooch.com/favicon.png\">\\n<title>Kevin J.Anderson : The X-Files (3) - Ground Zero</title>\\n<meta http-equiv=\"Content-Type\"')"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ldata = data.values.tolist()\n",
        "ldata = [i[0] for i in ldata]\n",
        "ldata[0][:200], ldata[10][:300]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGBTg9g4DMVJ"
      },
      "source": [
        "#### Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "nbpM3QUwDMVK"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
        "import re\n",
        "# nltk.download('words')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download(\"wordnet\")\n",
        "# nltk.download('omw-1.4') \n",
        "# nltk.download('punkt') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "Hklvhb6RDMVL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' Eric Newby : Short Walk in the Hindu KushEric Newby : Short Walk in the Hindu Kush?                                                    Author:                        Eric Newby                                    Title:                        Short Walk in the Hindu Kush                            M'"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1. Remove html-tags\n",
        "html_reg = \"\"\"<(\"[^\"]*\"|'[^']*'|[^'\">])*>\"\"\"\n",
        "other_reg = \"\"\n",
        "\n",
        "tldata = []\n",
        "for item in ldata:\n",
        "\n",
        "    nitem = item\n",
        "    nitem = re.sub(html_reg, \"\", nitem)\n",
        "    nitem = re.sub(\"\\t\", \"\", nitem)\n",
        "    nitem = re.sub(\"\\r\", \"\", nitem)\n",
        "    nitem = re.sub(\"\\n\", \"\", nitem)\n",
        "    nitem = re.sub(\"&nbsp;\", \"\", nitem)\n",
        "\n",
        "    tldata.append(nitem)\n",
        "    \n",
        "    continue\n",
        "\n",
        "ldata = tldata\n",
        "ldata[1][:300]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['eric',\n",
              " 'newby',\n",
              " ':',\n",
              " 'short',\n",
              " 'walk',\n",
              " 'in',\n",
              " 'the',\n",
              " 'hindu',\n",
              " 'kusheric',\n",
              " 'newby',\n",
              " ':',\n",
              " 'short',\n",
              " 'walk',\n",
              " 'in',\n",
              " 'the',\n",
              " 'hindu',\n",
              " 'kush',\n",
              " '?',\n",
              " 'author',\n",
              " ':',\n",
              " 'eric',\n",
              " 'newby',\n",
              " 'title',\n",
              " ':',\n",
              " 'short',\n",
              " 'walk',\n",
              " 'in',\n",
              " 'the',\n",
              " 'hindu',\n",
              " 'kush',\n",
              " 'moochablecopies',\n",
              " ':',\n",
              " 'no',\n",
              " 'copies',\n",
              " 'available',\n",
              " 'recommended',\n",
              " ':',\n",
              " 'the',\n",
              " 'trees',\n",
              " 'of',\n",
              " 'the',\n",
              " 'world',\n",
              " ':',\n",
              " 'a',\n",
              " 'dozen',\n",
              " 'padded',\n",
              " 'envelopesasne',\n",
              " 'seierstad',\n",
              " ':',\n",
              " 'the',\n",
              " 'bookseller',\n",
              " 'of',\n",
              " 'kabulalice',\n",
              " 'sebold',\n",
              " ':',\n",
              " 'the',\n",
              " 'lovely',\n",
              " 'bonesgreg',\n",
              " 'mortenson',\n",
              " 'and',\n",
              " 'david',\n",
              " 'o',\n",
              " '...',\n",
              " ':',\n",
              " 'three',\n",
              " 'cups',\n",
              " 'of',\n",
              " 'tea',\n",
              " ':',\n",
              " 'one',\n",
              " 'man',\n",
              " \"'s\",\n",
              " 'mission',\n",
              " '...',\n",
              " 'muriel',\n",
              " 'barbery',\n",
              " ':',\n",
              " 'the',\n",
              " 'elegance',\n",
              " 'of',\n",
              " 'the',\n",
              " 'hedgehogjon',\n",
              " 'krakauer',\n",
              " ':',\n",
              " 'under',\n",
              " 'the',\n",
              " 'banner',\n",
              " 'of',\n",
              " 'heaven',\n",
              " ':',\n",
              " 'a',\n",
              " 'story',\n",
              " '...',\n",
              " 'pat',\n",
              " 'parker',\n",
              " ':',\n",
              " 'unleashing',\n",
              " 'feminism',\n",
              " ':',\n",
              " 'critiquing']"
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## intermediate processing\n",
        "\n",
        "# to low each character\n",
        "tldata = []\n",
        "for item in ldata:\n",
        "    tldata.append(item.lower())\n",
        "    continue\n",
        "ldata = tldata\n",
        "\n",
        "# split string to words\n",
        "tldata = []\n",
        "for item in ldata:\n",
        "    # tldata.append(item.split(' '))\n",
        "    # tmp = []\n",
        "    tldata.append([i for i in nltk.word_tokenize(item)])\n",
        "\n",
        "    continue\n",
        "ldata = tldata\n",
        "\n",
        "# delete empty items\n",
        "tldata = []\n",
        "for words in ldata:\n",
        "    tldata.append([word for word in words if word != ''])\n",
        "    continue\n",
        "ldata = tldata\n",
        "\n",
        "tldata[1][:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "H59-VAGmDMVM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "874\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "874"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2. Remove non-english words\n",
        "eng_words = set(nltk.corpus.words.words())\n",
        "\n",
        "tldata = []\n",
        "print(len(ldata[1]))\n",
        "for words in ldata:\n",
        "    # tldata.append(\" \".join(w for w in nltk.wordpunct_tokenize(item) if w.lower() in eng_words or not w.isalpha()))\n",
        "    tldata.append(w for w in words if w.lower() in eng_words or not w.isalpha())\n",
        "    continue\n",
        "\n",
        "ldata = tldata\n",
        "len(ldata[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 3. Remove stop-words\n",
        "\n",
        "# tldata = []\n",
        "# for words in ldata:\n",
        "#     print(len(words))\n",
        "#     tmp = [word for word in words if word not in stopwords.words()]\n",
        "#     tldata.append(tmp)\n",
        "#     continue\n",
        "\n",
        "# len(words)\n",
        "# ldata = tldata\n",
        "# ldata[1][:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['eric',\n",
              " 'newby',\n",
              " ':',\n",
              " 'short',\n",
              " 'walk',\n",
              " 'in',\n",
              " 'the',\n",
              " 'hindu',\n",
              " 'kusheric',\n",
              " 'newby']"
            ]
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4. Apply lemmatization / stemming\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "tldata = []\n",
        "for words in ldata:\n",
        "    tldata.append([ i for i in [lemmatizer.lemmatize(i) for i in words] if i != '' ])\n",
        "    # [stemmer.stem(i) for i in words]\n",
        "    continue\n",
        "\n",
        "ldata = tldata\n",
        "ldata[1][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Additional processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h_c-1kBDMVN"
      },
      "source": [
        "#### Vizualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTu83iAWDMVN"
      },
      "source": [
        "As a visualisation, it is necessary to construct a frequency distribution of words (the 100 most common words), sorted by frequency. \n",
        "\n",
        "For visualization purposes we advice you to use plotly, but you are free to choose other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "bHX6IXcrDMVO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['eric',\n",
              "  'newby',\n",
              "  ':',\n",
              "  'short',\n",
              "  'walk',\n",
              "  'in',\n",
              "  'the',\n",
              "  'hindu',\n",
              "  'kusheric',\n",
              "  'newby',\n",
              "  ':',\n",
              "  'short',\n",
              "  'walk',\n",
              "  'in',\n",
              "  'the',\n",
              "  'hindu',\n",
              "  'kush',\n",
              "  '?',\n",
              "  'author',\n",
              "  ':',\n",
              "  'eric',\n",
              "  'newby',\n",
              "  'title',\n",
              "  ':',\n",
              "  'short',\n",
              "  'walk',\n",
              "  'in',\n",
              "  'the',\n",
              "  'hindu',\n",
              "  'kush',\n",
              "  'moochablecopies',\n",
              "  ':',\n",
              "  'no',\n",
              "  'copy',\n",
              "  'available',\n",
              "  'recommended',\n",
              "  ':',\n",
              "  'the',\n",
              "  'tree',\n",
              "  'of',\n",
              "  'the',\n",
              "  'world',\n",
              "  ':',\n",
              "  'a',\n",
              "  'dozen',\n",
              "  'padded',\n",
              "  'envelopesasne',\n",
              "  'seierstad',\n",
              "  ':',\n",
              "  'the',\n",
              "  'bookseller',\n",
              "  'of',\n",
              "  'kabulalice',\n",
              "  'sebold',\n",
              "  ':',\n",
              "  'the',\n",
              "  'lovely',\n",
              "  'bonesgreg',\n",
              "  'mortenson',\n",
              "  'and',\n",
              "  'david',\n",
              "  'o',\n",
              "  '...',\n",
              "  ':',\n",
              "  'three',\n",
              "  'cup',\n",
              "  'of',\n",
              "  'tea',\n",
              "  ':',\n",
              "  'one',\n",
              "  'man',\n",
              "  \"'s\",\n",
              "  'mission',\n",
              "  '...',\n",
              "  'muriel',\n",
              "  'barbery',\n",
              "  ':',\n",
              "  'the',\n",
              "  'elegance',\n",
              "  'of',\n",
              "  'the',\n",
              "  'hedgehogjon',\n",
              "  'krakauer',\n",
              "  ':',\n",
              "  'under',\n",
              "  'the',\n",
              "  'banner',\n",
              "  'of',\n",
              "  'heaven',\n",
              "  ':',\n",
              "  'a',\n",
              "  'story',\n",
              "  '...',\n",
              "  'pat',\n",
              "  'parker',\n",
              "  ':',\n",
              "  'unleashing',\n",
              "  'feminism',\n",
              "  ':',\n",
              "  'critiquing',\n",
              "  'lesb',\n",
              "  '...',\n",
              "  'alexander',\n",
              "  'mccall',\n",
              "  'smith',\n",
              "  ':',\n",
              "  'tear',\n",
              "  'of',\n",
              "  'the',\n",
              "  'giraffedava',\n",
              "  'sobel',\n",
              "  ':',\n",
              "  'longitude',\n",
              "  ':',\n",
              "  'the',\n",
              "  'true',\n",
              "  'story',\n",
              "  'of',\n",
              "  'a',\n",
              "  'lone',\n",
              "  '...',\n",
              "  'joanne',\n",
              "  'harris',\n",
              "  ':',\n",
              "  'five',\n",
              "  'quarter',\n",
              "  'of',\n",
              "  'the',\n",
              "  'orange',\n",
              "  '--',\n",
              "  '-show',\n",
              "  'more',\n",
              "  'recommendation',\n",
              "  '...',\n",
              "  '>',\n",
              "  'topic',\n",
              "  ':',\n",
              "  'afghanistanasiabombaycalcuttadelhieducation',\n",
              "  '&',\n",
              "  'referenceessays',\n",
              "  '&',\n",
              "  'traveloguesindiareference',\n",
              "  '&',\n",
              "  'tipstravelwriting',\n",
              "  '>',\n",
              "  'publishedin',\n",
              "  ':',\n",
              "  'english',\n",
              "  'binding',\n",
              "  ':',\n",
              "  'audio',\n",
              "  'cassette',\n",
              "  'page',\n",
              "  ':',\n",
              "  'date',\n",
              "  ':',\n",
              "  '1995-10-23',\n",
              "  'isbn',\n",
              "  ':',\n",
              "  '0001049313',\n",
              "  'publisher',\n",
              "  ':',\n",
              "  'harpercollins',\n",
              "  'audio',\n",
              "  'weight:0.31',\n",
              "  'poundssize:4.17',\n",
              "  'x',\n",
              "  '5.28',\n",
              "  'x',\n",
              "  '0.71',\n",
              "  'inch',\n",
              "  'edition',\n",
              "  ':',\n",
              "  'abridged',\n",
              "  'amazonprices',\n",
              "  ':',\n",
              "  'wishlists',\n",
              "  ':',\n",
              "  '3margaret',\n",
              "  'h.',\n",
              "  '(',\n",
              "  'usa',\n",
              "  ':',\n",
              "  'nm',\n",
              "  ')',\n",
              "  ',',\n",
              "  'sara',\n",
              "  '(',\n",
              "  'singapore',\n",
              "  ')',\n",
              "  ',',\n",
              "  'cej10',\n",
              "  '(',\n",
              "  'usa',\n",
              "  ':',\n",
              "  'ia',\n",
              "  ')',\n",
              "  '.',\n",
              "  'description',\n",
              "  ':',\n",
              "  'product',\n",
              "  'descriptioneric',\n",
              "  'newby',\n",
              "  'describes',\n",
              "  'his',\n",
              "  'travel',\n",
              "  'in',\n",
              "  'the',\n",
              "  'mountain',\n",
              "  'of',\n",
              "  'afghanistan',\n",
              "  '.',\n",
              "  'he',\n",
              "  'ha',\n",
              "  'also',\n",
              "  'written',\n",
              "  '``',\n",
              "  'the',\n",
              "  'last',\n",
              "  'grain',\n",
              "  'race',\n",
              "  \"''\",\n",
              "  ',',\n",
              "  '``',\n",
              "  'slowly',\n",
              "  'down',\n",
              "  'the',\n",
              "  'ganges',\n",
              "  \"''\",\n",
              "  ',',\n",
              "  '``',\n",
              "  'love',\n",
              "  'and',\n",
              "  'war',\n",
              "  'in',\n",
              "  'the',\n",
              "  'apennines',\n",
              "  \"''\",\n",
              "  'and',\n",
              "  '``',\n",
              "  'on',\n",
              "  'the',\n",
              "  'shore',\n",
              "  'of',\n",
              "  'the',\n",
              "  'mediterranean',\n",
              "  \"''\",\n",
              "  '.amazon.com',\n",
              "  'reviewfor',\n",
              "  'more',\n",
              "  'than',\n",
              "  'a',\n",
              "  'decade',\n",
              "  'following',\n",
              "  'the',\n",
              "  'end',\n",
              "  'of',\n",
              "  'world',\n",
              "  'war',\n",
              "  'ii',\n",
              "  ',',\n",
              "  'eric',\n",
              "  'newby',\n",
              "  'toiled',\n",
              "  'away',\n",
              "  'in',\n",
              "  'the',\n",
              "  'british',\n",
              "  'fashion',\n",
              "  'industry',\n",
              "  ',',\n",
              "  'peddling',\n",
              "  'some',\n",
              "  'of',\n",
              "  'the',\n",
              "  'ugliest',\n",
              "  'clothes',\n",
              "  'on',\n",
              "  'the',\n",
              "  'planet',\n",
              "  '.',\n",
              "  '(',\n",
              "  'regarding',\n",
              "  'one',\n",
              "  'wafer-thin',\n",
              "  'model',\n",
              "  'in',\n",
              "  'her',\n",
              "  'runway',\n",
              "  'best',\n",
              "  ',',\n",
              "  'he',\n",
              "  'wa',\n",
              "  'reminded',\n",
              "  'of',\n",
              "  '``',\n",
              "  'those',\n",
              "  'flagpole',\n",
              "  'they',\n",
              "  'put',\n",
              "  'up',\n",
              "  'in',\n",
              "  'the',\n",
              "  'mall',\n",
              "  'when',\n",
              "  'the',\n",
              "  'queen',\n",
              "  'come',\n",
              "  'home',\n",
              "  '.',\n",
              "  \"''\",\n",
              "  ')',\n",
              "  'fortunately',\n",
              "  ',',\n",
              "  'newby',\n",
              "  'reached',\n",
              "  'the',\n",
              "  'end',\n",
              "  'his',\n",
              "  'haute-couture',\n",
              "  'tether',\n",
              "  'in',\n",
              "  '1956.',\n",
              "  'at',\n",
              "  'that',\n",
              "  'point',\n",
              "  ',',\n",
              "  'with',\n",
              "  'the',\n",
              "  'sort',\n",
              "  'of',\n",
              "  'sublime',\n",
              "  'impulsiveness',\n",
              "  'that',\n",
              "  \"'s\",\n",
              "  'forbidden',\n",
              "  'to',\n",
              "  'fictional',\n",
              "  'character',\n",
              "  'but',\n",
              "  'endemic',\n",
              "  'to',\n",
              "  'real',\n",
              "  'one',\n",
              "  ',',\n",
              "  'he',\n",
              "  'decided',\n",
              "  'to',\n",
              "  'visit',\n",
              "  'a',\n",
              "  'remote',\n",
              "  'corner',\n",
              "  'of',\n",
              "  'afghanistan',\n",
              "  ',',\n",
              "  'where',\n",
              "  'no',\n",
              "  'englishman',\n",
              "  'had',\n",
              "  'planted',\n",
              "  'his',\n",
              "  'brogan',\n",
              "  'for',\n",
              "  'at',\n",
              "  'least',\n",
              "  '50',\n",
              "  'year',\n",
              "  '.',\n",
              "  'what',\n",
              "  \"'s\",\n",
              "  'more',\n",
              "  ',',\n",
              "  'he',\n",
              "  'recorded',\n",
              "  'his',\n",
              "  'adventure',\n",
              "  'in',\n",
              "  'a',\n",
              "  'classic',\n",
              "  'narrative',\n",
              "  ',',\n",
              "  'a',\n",
              "  'short',\n",
              "  'walk',\n",
              "  'in',\n",
              "  'the',\n",
              "  'hindu',\n",
              "  'kush',\n",
              "  '.',\n",
              "  'the',\n",
              "  'title',\n",
              "  ',',\n",
              "  'of',\n",
              "  'course',\n",
              "  ',',\n",
              "  'is',\n",
              "  'a',\n",
              "  'fine',\n",
              "  'example',\n",
              "  'of',\n",
              "  'newby',\n",
              "  \"'s\",\n",
              "  'habitual',\n",
              "  'self-effacement',\n",
              "  ',',\n",
              "  'since',\n",
              "  'his',\n",
              "  'journey',\n",
              "  '--',\n",
              "  'which',\n",
              "  'included',\n",
              "  'a',\n",
              "  'near-ascent',\n",
              "  'of',\n",
              "  'the',\n",
              "  '19,800-foot',\n",
              "  'mir',\n",
              "  'samir',\n",
              "  '--',\n",
              "  'wa',\n",
              "  'anything',\n",
              "  'but',\n",
              "  'short',\n",
              "  '.',\n",
              "  'and',\n",
              "  'his',\n",
              "  'book',\n",
              "  'seems',\n",
              "  'to',\n",
              "  'furnish',\n",
              "  'a',\n",
              "  'missing',\n",
              "  'link',\n",
              "  'between',\n",
              "  'the',\n",
              "  'great',\n",
              "  'britannic',\n",
              "  'wanderer',\n",
              "  'of',\n",
              "  'the',\n",
              "  'victorian',\n",
              "  'era',\n",
              "  'and',\n",
              "  'such',\n",
              "  'contemporary',\n",
              "  'jungle',\n",
              "  'nut',\n",
              "  'a',\n",
              "  'redmond',\n",
              "  \"o'hanlon\",\n",
              "  '.',\n",
              "  'at',\n",
              "  'time',\n",
              "  'it',\n",
              "  'also',\n",
              "  'brings',\n",
              "  'to',\n",
              "  'mind',\n",
              "  'evelyn',\n",
              "  'waugh',\n",
              "  ',',\n",
              "  'who',\n",
              "  'contributed',\n",
              "  'the',\n",
              "  'preface',\n",
              "  '.',\n",
              "  'newby',\n",
              "  'is',\n",
              "  'a',\n",
              "  'le',\n",
              "  'acidulous',\n",
              "  'writer',\n",
              "  ',',\n",
              "  'to',\n",
              "  'be',\n",
              "  'sure',\n",
              "  ',',\n",
              "  'and',\n",
              "  'he',\n",
              "  'ha',\n",
              "  'little',\n",
              "  'interest',\n",
              "  'in',\n",
              "  'launching',\n",
              "  'the',\n",
              "  'sort',\n",
              "  'of',\n",
              "  'heat-seeking',\n",
              "  'satiric',\n",
              "  'missile',\n",
              "  'that',\n",
              "  'were',\n",
              "  'waugh',\n",
              "  \"'s\",\n",
              "  'specialty',\n",
              "  '.',\n",
              "  'still',\n",
              "  ',',\n",
              "  'a',\n",
              "  'short',\n",
              "  'walk',\n",
              "  'in',\n",
              "  'the',\n",
              "  'hindu',\n",
              "  'kush',\n",
              "  'is',\n",
              "  'a',\n",
              "  'hilarious',\n",
              "  'read',\n",
              "  '.',\n",
              "  'the',\n",
              "  'author',\n",
              "  'excels',\n",
              "  'at',\n",
              "  'the',\n",
              "  'dispiriting',\n",
              "  'snapshot',\n",
              "  ',',\n",
              "  'capturing',\n",
              "  ',',\n",
              "  'say',\n",
              "  ',',\n",
              "  'the',\n",
              "  'afghan',\n",
              "  'backwater',\n",
              "  'of',\n",
              "  'fariman',\n",
              "  'in',\n",
              "  'two',\n",
              "  'crisp',\n",
              "  'sentence',\n",
              "  ':',\n",
              "  '``',\n",
              "  'a',\n",
              "  'whole',\n",
              "  'gale',\n",
              "  'of',\n",
              "  'wind',\n",
              "  'wa',\n",
              "  'blowing',\n",
              "  ',',\n",
              "  'tearing',\n",
              "  'up',\n",
              "  'the',\n",
              "  'surface',\n",
              "  'of',\n",
              "  'the',\n",
              "  'main',\n",
              "  'street',\n",
              "  '.',\n",
              "  'except',\n",
              "  'for',\n",
              "  'two',\n",
              "  'policeman',\n",
              "  'holding',\n",
              "  'hand',\n",
              "  'and',\n",
              "  'a',\n",
              "  'dog',\n",
              "  'whose',\n",
              "  'hind',\n",
              "  'leg',\n",
              "  'were',\n",
              "  'paralysed',\n",
              "  'it',\n",
              "  'wa',\n",
              "  'deserted',\n",
              "  '.',\n",
              "  \"''\",\n",
              "  'his',\n",
              "  'capsule',\n",
              "  'history',\n",
              "  'of',\n",
              "  'nuristan',\n",
              "  'also',\n",
              "  'get',\n",
              "  'in',\n",
              "  'some',\n",
              "  'sly',\n",
              "  'dig',\n",
              "  'at',\n",
              "  'britain',\n",
              "  \"'s\",\n",
              "  'special',\n",
              "  'relationship',\n",
              "  'with',\n",
              "  'the',\n",
              "  'violence-prone',\n",
              "  'abdur',\n",
              "  'rahman',\n",
              "  ':',\n",
              "  'officially',\n",
              "  'his',\n",
              "  'subsidy',\n",
              "  'had',\n",
              "  'just',\n",
              "  'been',\n",
              "  'increased',\n",
              "  'from',\n",
              "  '12,000',\n",
              "  'to',\n",
              "  '16,000',\n",
              "  'lakh',\n",
              "  'of',\n",
              "  'rupee',\n",
              "  '.',\n",
              "  'to',\n",
              "  'the',\n",
              "  'british',\n",
              "  'he',\n",
              "  'had',\n",
              "  'fully',\n",
              "  'justified',\n",
              "  'their',\n",
              "  'selection',\n",
              "  'of',\n",
              "  'him',\n",
              "  'a',\n",
              "  'amir',\n",
              "  'of',\n",
              "  'afghanistan',\n",
              "  'and',\n",
              "  ',',\n",
              "  'apart',\n",
              "  'from',\n",
              "  'the',\n",
              "  'few',\n",
              "  'foible',\n",
              "  'remarked',\n",
              "  'by',\n",
              "  'lord',\n",
              "  'curzon',\n",
              "  ',',\n",
              "  'like',\n",
              "  'flaying',\n",
              "  'people',\n",
              "  'alive',\n",
              "  'who',\n",
              "  'displeased',\n",
              "  'him',\n",
              "  ',',\n",
              "  'blowing',\n",
              "  'them',\n",
              "  'from',\n",
              "  'the',\n",
              "  'mouth',\n",
              "  'of',\n",
              "  'cannon',\n",
              "  ',',\n",
              "  'or',\n",
              "  'standing',\n",
              "  'them',\n",
              "  'up',\n",
              "  'to',\n",
              "  'the',\n",
              "  'neck',\n",
              "  'in',\n",
              "  'pool',\n",
              "  'of',\n",
              "  'water',\n",
              "  'on',\n",
              "  'the',\n",
              "  'summit',\n",
              "  'of',\n",
              "  'high',\n",
              "  'mountain',\n",
              "  'and',\n",
              "  'letting',\n",
              "  'them',\n",
              "  'freeze',\n",
              "  'solid',\n",
              "  ',',\n",
              "  'he',\n",
              "  'had',\n",
              "  'done',\n",
              "  'nothing',\n",
              "  'to',\n",
              "  'which',\n",
              "  'exception',\n",
              "  'could',\n",
              "  'be',\n",
              "  'taken',\n",
              "  '.',\n",
              "  'newby',\n",
              "  'also',\n",
              "  'surpasses',\n",
              "  'waugh',\n",
              "  '--',\n",
              "  'and',\n",
              "  'indeed',\n",
              "  ',',\n",
              "  'most',\n",
              "  'other',\n",
              "  'travel',\n",
              "  'writer',\n",
              "  '--',\n",
              "  'in',\n",
              "  'another',\n",
              "  'important',\n",
              "  'respect',\n",
              "  ':',\n",
              "  'he',\n",
              "  \"'s\",\n",
              "  'miraculously',\n",
              "  'free',\n",
              "  'of',\n",
              "  'solipsism',\n",
              "  '.',\n",
              "  'even',\n",
              "  'the',\n",
              "  'keenest',\n",
              "  'literary',\n",
              "  'voyager',\n",
              "  'tend',\n",
              "  'to',\n",
              "  'be',\n",
              "  ',',\n",
              "  'in',\n",
              "  'the',\n",
              "  'purest',\n",
              "  'sense',\n",
              "  'of',\n",
              "  'the',\n",
              "  'term',\n",
              "  ',',\n",
              "  'self-centered',\n",
              "  '.',\n",
              "  'but',\n",
              "  'a',\n",
              "  'short',\n",
              "  'walk',\n",
              "  'in',\n",
              "  'the',\n",
              "  'hindu',\n",
              "  'kush',\n",
              "  'includes',\n",
              "  'wonderfully',\n",
              "  'oblique',\n",
              "  'portrait',\n",
              "  'of',\n",
              "  'the',\n",
              "  'author',\n",
              "  \"'s\",\n",
              "  'travel',\n",
              "  'companion',\n",
              "  ',',\n",
              "  'hugh',\n",
              "  'carless',\n",
              "  ',',\n",
              "  'and',\n",
              "  'his',\n",
              "  'wife',\n",
              "  ',',\n",
              "  'wanda',\n",
              "  '(',\n",
              "  'who',\n",
              "  'play',\n",
              "  'a',\n",
              "  'starring',\n",
              "  'role',\n",
              "  'in',\n",
              "  'such',\n",
              "  'subsequent',\n",
              "  'chronicle',\n",
              "  'a',\n",
              "  'slowly',\n",
              "  'down',\n",
              "  'the',\n",
              "  'ganges',\n",
              "  ')',\n",
              "  '.',\n",
              "  'there',\n",
              "  'are',\n",
              "  'also',\n",
              "  'dozen',\n",
              "  'of',\n",
              "  'brilliant',\n",
              "  'cameo',\n",
              "  'part',\n",
              "  ',',\n",
              "  'and',\n",
              "  'an',\n",
              "  'indelible',\n",
              "  'record',\n",
              "  'of',\n",
              "  'a',\n",
              "  'stunning',\n",
              "  'landscape',\n",
              "  '.',\n",
              "  'the',\n",
              "  'roof',\n",
              "  'of',\n",
              "  'the',\n",
              "  'world',\n",
              "  'is',\n",
              "  ',',\n",
              "  'in',\n",
              "  'newby',\n",
              "  \"'s\",\n",
              "  'rendering',\n",
              "  ',',\n",
              "  'both',\n",
              "  'an',\n",
              "  'absolute',\n",
              "  'heaven',\n",
              "  'and',\n",
              "  'a',\n",
              "  'low-oxygen',\n",
              "  'hell',\n",
              "  '.',\n",
              "  'yet',\n",
              "  'the',\n",
              "  'author',\n",
              "  'never',\n",
              "  'pretend',\n",
              "  'to',\n",
              "  'pit',\n",
              "  'himself',\n",
              "  'against',\n",
              "  'a',\n",
              "  'malicious',\n",
              "  'nature',\n",
              "  '--',\n",
              "  'his',\n",
              "  'mountain',\n",
              "  'are',\n",
              "  ',',\n",
              "  'in',\n",
              "  'frost',\n",
              "  \"'s\",\n",
              "  'memorable',\n",
              "  'phrase',\n",
              "  ',',\n",
              "  'too',\n",
              "  'lofty',\n",
              "  'and',\n",
              "  'original',\n",
              "  'to',\n",
              "  'rage',\n",
              "  '.',\n",
              "  'which',\n",
              "  'is',\n",
              "  'yet',\n",
              "  'another',\n",
              "  'reason',\n",
              "  'to',\n",
              "  'call',\n",
              "  'this',\n",
              "  'little',\n",
              "  'masterpiece',\n",
              "  'a',\n",
              "  'peak',\n",
              "  'performance',\n",
              "  '.',\n",
              "  '--',\n",
              "  'james',\n",
              "  'marcus',\n",
              "  'url',\n",
              "  ':',\n",
              "  'http',\n",
              "  ':',\n",
              "  '//bookmooch.com/0001049313',\n",
              "  'wishlistadd',\n",
              "  '>',\n",
              "  'saveforlater',\n",
              "  '>',\n",
              "  'amazon',\n",
              "  '>',\n",
              "  'otherwebsites',\n",
              "  '>',\n",
              "  'relatededitions',\n",
              "  '>',\n",
              "  'recommend',\n",
              "  '>'],\n",
              " 0]"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wordfreq1 = [[word, words.count(word)] for word in ldata ]\n",
        "wordfreq1[1][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_wordfreq(list, limit=100):\n",
        "    fwordfreq = []\n",
        "    for word in list:\n",
        "\n",
        "        fwordfreq.append([word, int(list.count(word))])\n",
        "\n",
        "        if(len(fwordfreq) >= limit):\n",
        "            break\n",
        "        \n",
        "        continue\n",
        "    \n",
        "    fwordfreq = pd.DataFrame(fwordfreq, columns=['word', 'freq'])\n",
        "    fwordfreq.drop_duplicates(inplace=True)\n",
        "\n",
        "    return fwordfreq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>:</td>\n",
              "      <td>2023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>&gt;</td>\n",
              "      <td>764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>the</td>\n",
              "      <td>557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>&amp;</td>\n",
              "      <td>418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>of</td>\n",
              "      <td>403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>frontedition</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>europeflorencehiroshima</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>cassetteeastern</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>apennineseric</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>abridgedancientasiaaudiobooksbiographies</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>61 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        word  freq\n",
              "2                                          :  2023\n",
              "59                                         >   764\n",
              "7                                        the   557\n",
              "78                                         &   418\n",
              "49                                        of   403\n",
              "..                                       ...   ...\n",
              "86                              frontedition     1\n",
              "90                   europeflorencehiroshima     1\n",
              "85                           cassetteeastern     1\n",
              "8                              apennineseric     1\n",
              "77  abridgedancientasiaaudiobooksbiographies     1\n",
              "\n",
              "[61 rows x 2 columns]"
            ]
          },
          "execution_count": 180,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# init flatten list of words\n",
        "fldata = []\n",
        "for sublist in ldata:\n",
        "    for item in sublist:\n",
        "        # if item not in fldata:\n",
        "        fldata.append(item)\n",
        "        continue\n",
        "    continue\n",
        "\n",
        "fwordfreq = get_wordfreq(fldata)\n",
        "# fwordfreq = fwordfreq[fwordfreq[:, 1].argsort()[::-1]]\n",
        "fwordfreq.sort_values(by=['freq', 'word'], ascending=False, inplace=True)\n",
        "fwordfreq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_wordfreq(wordfreq, limit=30):\n",
        "\n",
        "    data = wordfreq[:limit]\n",
        "    # print(data)\n",
        "\n",
        "    # df = pd.DataFrame({'c1':['apple','banana','banana'],'c2':['orange','apple','apple'],'c3':['banana','date','orange']})\n",
        "    # df_stack = wordfreq.stack()\n",
        "    # counter = df_stack.value_counts() # set top 10: df_stack.value_counts()[0:10]\n",
        "    # plt.bar(counter.index, counter.values)\n",
        "\n",
        "    # plt.hist(wordfreq)\n",
        "    # plt.barh(wordfreq['freq'], width=2000, height=100)\n",
        "\n",
        "    # fig = plt.figure(figsize=(20,5))\n",
        "    # ax = fig.add_subplot(1,1,1)\n",
        "    # # ax.set_xticks([0,5,10])\n",
        "    # # ax.set_xticklabels([0,5,10])\n",
        "    # # ax.set_yticks([0,1])\n",
        "    # ax.set_yticklabels(wordfreq['word'])\n",
        "    # wordfreq['freq'].hist(ax=ax,orientation='horizontal')\n",
        "    # fig.tight_layout()  # Improves appearance a bit.\n",
        "\n",
        "    # Plot the figure.\n",
        "    # ax = plt.subplot(1,1,1)\n",
        "    ax = data.plot(kind=\"bar\")\n",
        "    # ax.plot(wordfreq.plot(kind=\"bar\"))\n",
        "    ax.set_title(\"Amount Frequency\")\n",
        "    ax.set_xlabel(\"Words\")\n",
        "    ax.set_ylabel(\"Frequency\")\n",
        "    ax.set_xticklabels(data['word'])\n",
        "    plt.show()\n",
        "    pass\n",
        "\n",
        "\n",
        "show_wordfreq(fwordfreq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZxcPfLTDMVO"
      },
      "source": [
        "#### Provide examples of processed text (some parts)\n",
        "\n",
        "Is everything all right with the result of cleaning these examples? What kind of information was lost?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BSMfa9lDMVP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xn6XK7UDMVP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlSeYg4sDMVQ"
      },
      "source": [
        "# [10 points] Part 2. Duplicates detection. LSH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuMBHSiDDMVR"
      },
      "source": [
        "#### Libraries you can use\n",
        "\n",
        "1. LSH - https://github.com/ekzhu/datasketch\n",
        "1. LSH - https://github.com/mattilyra/LSH\n",
        "1. Any other library on your choise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uff5aBpgDMVR"
      },
      "source": [
        "1. Detect duplicated text (duplicates do not imply a complete word-to-word match, but texts that may contain a paraphrase, rearrangement of words, sentences)\n",
        "1. Make a plot dependency of duplicates on shingle size (with fixed minhash length) \n",
        "1. Make a plot dependency of duplicates on minhash length (with fixed shingle size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "QOFZ7AvyDMVS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Approximate neighbours with Jaccard similarity > 0.5 []\n"
          ]
        }
      ],
      "source": [
        "# raise Exception('nope')\n",
        "import datasketch\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "\n",
        "# 1. Detect duplicated text\n",
        "m1 = MinHash(num_perm=128)\n",
        "for word in fldata:\n",
        "    m1.update(word.encode('utf8'))\n",
        "    continue\n",
        "\n",
        "# Create LSH index\n",
        "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
        "result = lsh.query(m1)\n",
        "print(\"Approximate neighbours with Jaccard similarity > 0.5\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPonm9U3DMVS"
      },
      "outputs": [],
      "source": [
        "# 2. Make a plot dependency of duplicates on shingle size (with fixed minhash length) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Make a plot dependency of duplicates on minhash length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njdTa1daDMVS"
      },
      "source": [
        "# [Optional 10 points] Part 3. Topic model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BwJNkn2DMVT"
      },
      "source": [
        "In this part you will learn how to do topic modeling with common tools and assess the resulting quality of the models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "l0WcLMhHDMVT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path3 = './storage/hw1/data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5321</th>\n",
              "      <td>id16606</td>\n",
              "      <td>The story is that there's a whole legion of de...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5208</th>\n",
              "      <td>id19501</td>\n",
              "      <td>The wild eyes blazed with a too too glorious e...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11243</th>\n",
              "      <td>id19312</td>\n",
              "      <td>We resided principally in the latter, and the ...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19183</th>\n",
              "      <td>id01039</td>\n",
              "      <td>He wrote this in a very ancient hand, and when...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12846</th>\n",
              "      <td>id14532</td>\n",
              "      <td>Their number was past guessing.</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11568</th>\n",
              "      <td>id07523</td>\n",
              "      <td>\"Your affectionate and afflicted father, \"Alph...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3506</th>\n",
              "      <td>id07133</td>\n",
              "      <td>If we survive this coming summer, I will not s...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8352</th>\n",
              "      <td>id11358</td>\n",
              "      <td>This was precisely what had formed the subject...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16365</th>\n",
              "      <td>id21231</td>\n",
              "      <td>By the time that it wanted only three minutes ...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10606</th>\n",
              "      <td>id02509</td>\n",
              "      <td>My father, his face lined with sorrow, stood b...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id                                               text author\n",
              "5321   id16606  The story is that there's a whole legion of de...    HPL\n",
              "5208   id19501  The wild eyes blazed with a too too glorious e...    EAP\n",
              "11243  id19312  We resided principally in the latter, and the ...    MWS\n",
              "19183  id01039  He wrote this in a very ancient hand, and when...    HPL\n",
              "12846  id14532                    Their number was past guessing.    HPL\n",
              "...        ...                                                ...    ...\n",
              "11568  id07523  \"Your affectionate and afflicted father, \"Alph...    MWS\n",
              "3506   id07133  If we survive this coming summer, I will not s...    MWS\n",
              "8352   id11358  This was precisely what had formed the subject...    EAP\n",
              "16365  id21231  By the time that it wanted only three minutes ...    EAP\n",
              "10606  id02509  My father, his face lined with sorrow, stood b...    HPL\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "execution_count": 236,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data3 = pd.read_csv(path3)\n",
        "\n",
        "# for debug\n",
        "data3 = data3.sample(1000)\n",
        "\n",
        "data3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkNFzxqPDMVU"
      },
      "source": [
        "The provided data contain chunked stories by Edgar Allan Poe (EAP), Mary Shelley (MWS), and HP Lovecraft (HPL).\n",
        "\n",
        "The dataset can be downloaded here: `https://drive.google.com/file/d/14tAjAzHr6UmFVFV7ABTyNHBh-dWHAaLH/view?usp=sharing`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BNIa1miDMVU"
      },
      "source": [
        "#### Preprocess dataset with the functions from the Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "9B8PZNRyDMVV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(\"The story is that there's a whole legion of devils seen sometimes on that reef sprawled about, or darting in and out of some kind of caves near the top.\",\n",
              " 'Neither did his old physician Dr. Davis, who died years ago.')"
            ]
          },
          "execution_count": 239,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ldata3 = data3['text'].values.tolist()\n",
        "ldata3[0][:200], ldata3[10][:200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
        "import re\n",
        "# nltk.download('words')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download(\"wordnet\")\n",
        "# nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The wild eyes blazed with a too too glorious effulgence; the pale fingers became of the transparent waxen hue of the grave, and the blue veins upon the lofty forehead swelled and sank impetuously with the tides of the gentle emotion.'"
            ]
          },
          "execution_count": 241,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1. Remove html-tags\n",
        "html_reg = \"\"\"<(\"[^\"]*\"|'[^']*'|[^'\">])*>\"\"\"\n",
        "other_reg = \"\"\n",
        "\n",
        "tldata = []\n",
        "for item in ldata3:\n",
        "\n",
        "    nitem = item\n",
        "    nitem = re.sub(html_reg, \"\", nitem)\n",
        "    nitem = re.sub(\"\\t\", \"\", nitem)\n",
        "    nitem = re.sub(\"\\r\", \"\", nitem)\n",
        "    nitem = re.sub(\"\\n\", \"\", nitem)\n",
        "    nitem = re.sub(\"&nbsp;\", \"\", nitem)\n",
        "\n",
        "    tldata.append(nitem)\n",
        "    \n",
        "    continue\n",
        "\n",
        "ldata3 = tldata\n",
        "ldata3[1][:300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the',\n",
              " 'wild',\n",
              " 'eyes',\n",
              " 'blazed',\n",
              " 'with',\n",
              " 'a',\n",
              " 'too',\n",
              " 'too',\n",
              " 'glorious',\n",
              " 'effulgence',\n",
              " ';',\n",
              " 'the',\n",
              " 'pale',\n",
              " 'fingers',\n",
              " 'became',\n",
              " 'of',\n",
              " 'the',\n",
              " 'transparent',\n",
              " 'waxen',\n",
              " 'hue',\n",
              " 'of',\n",
              " 'the',\n",
              " 'grave',\n",
              " ',',\n",
              " 'and',\n",
              " 'the',\n",
              " 'blue',\n",
              " 'veins',\n",
              " 'upon',\n",
              " 'the']"
            ]
          },
          "execution_count": 242,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## intermediate processing\n",
        "\n",
        "# to low each character\n",
        "tldata = []\n",
        "for item in ldata3:\n",
        "    tldata.append(item.lower())\n",
        "    continue\n",
        "ldata3 = tldata\n",
        "\n",
        "# split string to words\n",
        "tldata = []\n",
        "for item in ldata3:\n",
        "    # tldata.append(item.split(' '))\n",
        "    # tmp = []\n",
        "    tldata.append([i for i in nltk.word_tokenize(item)])\n",
        "\n",
        "    continue\n",
        "ldata3 = tldata\n",
        "\n",
        "# delete empty items\n",
        "tldata = []\n",
        "for words in ldata3:\n",
        "    tldata.append([word for word in words if word != ''])\n",
        "    continue\n",
        "ldata3 = tldata\n",
        "\n",
        "ldata3[1][:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "execution_count": 243,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2. Remove non-english words\n",
        "eng_words = set(nltk.corpus.words.words())\n",
        "\n",
        "tldata = []\n",
        "print(len(ldata3[1]))\n",
        "for words in ldata3:\n",
        "    # tldata.append(\" \".join(w for w in nltk.wordpunct_tokenize(item) if w.lower() in eng_words or not w.isalpha()))\n",
        "    tldata.append(w for w in words if w.lower() in eng_words or not w.isalpha())\n",
        "    continue\n",
        "\n",
        "ldata3 = ldata3\n",
        "len(ldata3[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "366"
            ]
          },
          "execution_count": 244,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3. Remove stop-words\n",
        "tldata = []\n",
        "for words in ldata:\n",
        "    tmp = [word for word in words if word not in stopwords.words()]\n",
        "    tldata.append(tmp)\n",
        "    continue\n",
        "\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9460\\4285249919.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mldata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtldata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mldata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# 4. Apply lemmatization / stemming\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "tldata = []\n",
        "for words in ldata:\n",
        "    tldata.append([lemmatizer.lemmatize(i) for i in words])\n",
        "    # [stemmer.stem(i) for i in words]\n",
        "    continue\n",
        "\n",
        "ldata = tldata\n",
        "ldata[1][100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKU2D29CDMVV"
      },
      "source": [
        "#### Quality estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6xfd3vpDMVW"
      },
      "source": [
        "Implement the following three quality fuctions: `coherence` (or `tf-idf coherence`), `normalized PMI`, `based on the distributed word representation`(you can use pretrained w2v vectors or some other model). You are free to use any libraries (for instance gensim) and components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim import topic_coherence\n",
        "from gensim.test.utils import common_corpus, common_dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "AEtv-gWcDMVW"
      },
      "outputs": [],
      "source": [
        "# def coherence(x, y, fs=1.0):\n",
        "#     Pxy, Pxx, Pyy = 0, 0, 0\n",
        "#     Cxy = abs(Pxy)**2/(Pxx*Pyy)\n",
        "#     return Cxy\n",
        "\n",
        "def coherence(topics):\n",
        "    cm = CoherenceModel(topics=topics, corpus=common_corpus, dictionary=common_dictionary, coherence='u_mass')\n",
        "    coherence = cm.get_coherence()  # get coherence value\n",
        "    return coherence\n",
        "\n",
        "# coherence(ldata[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalizedPMI():\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "unexpected EOF while parsing (3674980287.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\bogya\\AppData\\Local\\Temp\\ipykernel_14444\\3674980287.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def representationDistr():\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ],
      "source": [
        "def representationDistr():\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp1jTJmPDMVX"
      },
      "source": [
        "### Topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgbVtgphDMVX"
      },
      "source": [
        "Read and preprocess the dataset, divide it into train and test parts `sklearn.model_selection.train_test_split`. Test part will be used in classification part. For simplicity we do not perform cross-validation here, but you should remember about it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Uh0J_C8DMVY"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNepVz1wDMVY"
      },
      "source": [
        "Plot the histogram of resulting tokens counts in the processed datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_7JVEfbDMVZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulOjNL9vDMVZ"
      },
      "source": [
        "Plot the histogram of resulting tokens counts in the processed datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAse0LBvDMVa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N86AaGNDMVa"
      },
      "source": [
        "#### NMF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPelcAFQDMVa"
      },
      "source": [
        "Implement topic modeling with NMF (you can use `sklearn.decomposition.NMF`) and print out resulting topics. Try to change hyperparameters to better fit the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "DCPzCMMMDMVa"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import NMF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n",
        "\n",
        "model = NMF(n_components=2, init='random', random_state=0)\n",
        "W = model.fit_transform(X)\n",
        "H = model.components_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuaKAGIWDMVb"
      },
      "source": [
        "#### LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAK_MfQxDMVb"
      },
      "source": [
        "Implement topic modeling with LDA (you can use gensim implementation) and print out resulting topics. Try to change hyperparameters to better fit the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://radimrehurek.com/gensim/models/ldamodel.html\n",
        "from gensim.models import ldamulticore\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.test.utils import datapath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdzopuCJDMVb"
      },
      "outputs": [],
      "source": [
        "# Create a corpus from a list of texts\n",
        "common_dictionary = Dictionary(common_texts)\n",
        "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
        "\n",
        "# Train the model on the corpus.\n",
        "lda = LdaModel(common_corpus, num_topics=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSF6fq0NDMVc"
      },
      "source": [
        "### Additive regularization of topic models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVcA4itQDMVc"
      },
      "source": [
        "Implement topic modeling with ARTM. You may use bigartm library (simple installation for linux: pip install bigartm) or TopicNet framework (`https://github.com/machine-intelligence-laboratory/TopicNet`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03xhr0PmDMVc"
      },
      "source": [
        "Create artm topic model fit it to the data. Try to change hyperparameters (number of specific and background topics) to better fit the dataset. Play with smoothing and sparsing coefficients (use grid), try to add decorrelator. Print out resulting topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgv1qIUZDMVc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mteOYV8TDMVd"
      },
      "source": [
        "Write a function to convert new documents to topics probabilities vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGkD9YaxDMVd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPrNOdMLDMVd"
      },
      "source": [
        "Calculate the quality scores for each model. Make a barplot to compare the quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQadAQCXDMVd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of HW1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
