{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEEKQwtYDMUz"
      },
      "source": [
        "**SOFT DEADLINE:** `20.03.2022 23:59 msk` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-IAo9sYDMU-"
      },
      "source": [
        "# [5 points] Part 1. Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SNyjuGjDMU_"
      },
      "source": [
        "The task is to clear the text data of the crawled web-pages from different sites. \n",
        "\n",
        "It is necessary to ensure that the distribution of the 100 most frequent words includes only meaningful words in english language (not particles, conjunctions, prepositions, numbers, tags, symbols)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA4VkGl2DMVB"
      },
      "source": [
        "Determine the order of operations below and carry out the appropriate cleaning.\n",
        "\n",
        "1. Remove non-english words\n",
        "1. Remove html-tags (try to do it with regular expression, or play with beautifulsoap library)\n",
        "1. Apply lemmatization / stemming\n",
        "1. Remove stop-words\n",
        "1. Additional processing - At your own initiative, if this helps to obtain a better distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The choosen order:\n",
        "1. Remove html-tags\n",
        "2. Remove non-english words\n",
        "3. Remove stop-words\n",
        "4. Apply lemmatization / stemming\n",
        "5. Additional processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjLsB9pdDMVC"
      },
      "source": [
        "#### Hints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk6ZLncvDMVD"
      },
      "source": [
        "1. To do text processing you may use nltk and re libraries\n",
        "1. and / or any other libraries on your choise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCbTa1OiDMVE"
      },
      "source": [
        "#### Data reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXbG649IDMVG"
      },
      "source": [
        "The dataset for this part can be downloaded here: `https://drive.google.com/file/d/1wLwo83J-ikCCZY2RAoYx8NghaSaQ-lBA/view?usp=sharing`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# path = './storage/train.csv'\n",
        "path = './storage/web_sites_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "HN8UxSDkDMVH"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49158</th>\n",
              "      <td>&lt;!DOCTYPE html&gt;\\n  &lt;html class=\"desktop\"&gt;\\n\\n&lt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30189</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n&lt;!DOCTYPE ht...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30888</th>\n",
              "      <td>&lt;!-- START 'htmlHead' --&gt;\\n&lt;link rel=\"alternat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27006</th>\n",
              "      <td>&lt;HTML&gt;&lt;HEAD&gt;&lt;TITLE&gt;Quote for ED - FreeRealTime...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11917</th>\n",
              "      <td>&lt;HTML&gt;\\n&lt;HEAD&gt;\\n&lt;TITLE&gt;Lionel Messi  wallpaper...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35985</th>\n",
              "      <td>\\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7724</th>\n",
              "      <td>&lt;!-- START 'htmlHead' --&gt;\\n&lt;link rel=\"alternat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57195</th>\n",
              "      <td>\\n&lt;html&gt;\\n&lt;head&gt;\\n&lt;title&gt;FootyMania.com =&gt; Pla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54469</th>\n",
              "      <td>\\n&lt;html&gt;\\n&lt;head&gt;\\n&lt;title&gt;ESPNsoccernet.com Wor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2690</th>\n",
              "      <td>&lt;html&gt;\\n&lt;head&gt;\\n\\t&lt;title&gt;Barchart.com - Quote ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text\n",
              "49158  <!DOCTYPE html>\\n  <html class=\"desktop\">\\n\\n<...\n",
              "30189  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!DOCTYPE ht...\n",
              "30888  <!-- START 'htmlHead' -->\\n<link rel=\"alternat...\n",
              "27006  <HTML><HEAD><TITLE>Quote for ED - FreeRealTime...\n",
              "11917  <HTML>\\n<HEAD>\\n<TITLE>Lionel Messi  wallpaper...\n",
              "...                                                  ...\n",
              "35985  \\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0...\n",
              "7724   <!-- START 'htmlHead' -->\\n<link rel=\"alternat...\n",
              "57195  \\n<html>\\n<head>\\n<title>FootyMania.com => Pla...\n",
              "54469  \\n<html>\\n<head>\\n<title>ESPNsoccernet.com Wor...\n",
              "2690   <html>\\n<head>\\n\\t<title>Barchart.com - Quote ...\n",
              "\n",
              "[1000 rows x 1 columns]"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(path)\n",
        "\n",
        "# for debug\n",
        "data = data.sample(1000)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<!DOCTYPE html>\\n  <html class=\"desktop\">\\n\\n<head prefix=\"og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# good_reads: http://ogp.me/ns/fb/good_reads#\">\\n  <meta name=\"google-site-verification\" content=\"Pf',\n",
              " '<!-- START \\'htmlHead\\' -->\\n<link rel=\"alternate\" type=\"application/rss+xml\" title=\"SI - Top Stories [RSS]\" href=\"http://rss.cnn.com/rss/si_topstories.rss\"/>\\n<link rel=\"alternate\" type=\"application/rss+')"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ldata = data.values.tolist()\n",
        "ldata = [i[0] for i in ldata]\n",
        "ldata[0][:200], ldata[10][:200]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGBTg9g4DMVJ"
      },
      "source": [
        "#### Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "nbpM3QUwDMVK"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
        "import re\n",
        "# nltk.download('words')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download(\"wordnet\")\n",
        "# nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Hklvhb6RDMVL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'   @import \"http://uk.reuters.com/resources/css/rcom-main.css\";@import \"http://uk.reuters.com/resources/css/rcom-tertiary.css\";@import \"http://www.reuters.com/resources/css/rcom-homepage.css\";@import \"/includes/rcom-football.css\";@import \"/includes/cw-football.css\";            ClÃ¡udio CaÃ§apa | New'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1. Remove html-tags\n",
        "html_reg = \"\"\"<(\"[^\"]*\"|'[^']*'|[^'\">])*>\"\"\"\n",
        "other_reg = \"\"\n",
        "\n",
        "tldata = []\n",
        "for item in ldata:\n",
        "\n",
        "    nitem = item\n",
        "    nitem = re.sub(html_reg, \"\", nitem)\n",
        "    nitem = re.sub(\"\\t\", \"\", nitem)\n",
        "    nitem = re.sub(\"\\r\", \"\", nitem)\n",
        "    nitem = re.sub(\"\\n\", \"\", nitem)\n",
        "    nitem = re.sub(\"&nbsp;\", \"\", nitem)\n",
        "\n",
        "    tldata.append(nitem)\n",
        "    \n",
        "    continue\n",
        "\n",
        "ldata = tldata\n",
        "ldata[1][:300]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "## intermediate processing\n",
        "\n",
        "# split string to words\n",
        "tldata = []\n",
        "for item in ldata:\n",
        "    tldata.append(item.split(' '))\n",
        "    # [tldata.append(i) for i in nltk.word_tokenize(item)]\n",
        "    continue\n",
        "\n",
        "ldata = tldata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "H59-VAGmDMVM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1575\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1575"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2. Remove non-english words\n",
        "eng_words = set(nltk.corpus.words.words())\n",
        "\n",
        "tldata = []\n",
        "print(len(ldata[1]))\n",
        "for words in ldata:\n",
        "    # tldata.append(\" \".join(w for w in nltk.wordpunct_tokenize(item) if w.lower() in eng_words or not w.isalpha()))\n",
        "    tldata.append(w for w in words if w.lower() in eng_words or not w.isalpha())\n",
        "    continue\n",
        "\n",
        "tldata = ldata\n",
        "len(ldata[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Remove stop-words\n",
        "\n",
        "tldata = []\n",
        "for words in ldata:\n",
        "    tmp = [word for word in words if word not in stopwords.words()]\n",
        "    tldata.append(tmp)\n",
        "    continue\n",
        "\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4. Apply lemmatization / stemming\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "tldata = []\n",
        "for words in ldata:\n",
        "    tldata.append([lemmatizer.lemmatize(i) for i in words])\n",
        "    # [stemmer.stem(i) for i in words]\n",
        "    continue\n",
        "\n",
        "ldata = tldata\n",
        "ldata[1][100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Additional processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h_c-1kBDMVN"
      },
      "source": [
        "#### Vizualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTu83iAWDMVN"
      },
      "source": [
        "As a visualisation, it is necessary to construct a frequency distribution of words (the 100 most common words), sorted by frequency. \n",
        "\n",
        "For visualization purposes we advice you to use plotly, but you are free to choose other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHX6IXcrDMVO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZxcPfLTDMVO"
      },
      "source": [
        "#### Provide examples of processed text (some parts)\n",
        "\n",
        "Is everything all right with the result of cleaning these examples? What kind of information was lost?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BSMfa9lDMVP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xn6XK7UDMVP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlSeYg4sDMVQ"
      },
      "source": [
        "# [10 points] Part 2. Duplicates detection. LSH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuMBHSiDDMVR"
      },
      "source": [
        "#### Libraries you can use\n",
        "\n",
        "1. LSH - https://github.com/ekzhu/datasketch\n",
        "1. LSH - https://github.com/mattilyra/LSH\n",
        "1. Any other library on your choise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uff5aBpgDMVR"
      },
      "source": [
        "1. Detect duplicated text (duplicates do not imply a complete word-to-word match, but texts that may contain a paraphrase, rearrangement of words, sentences)\n",
        "1. Make a plot dependency of duplicates on shingle size (with fixed minhash length) \n",
        "1. Make a plot dependency of duplicates on minhash length (with fixed shingle size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOFZ7AvyDMVS"
      },
      "outputs": [],
      "source": [
        "# 1. Detect duplicated text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPonm9U3DMVS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njdTa1daDMVS"
      },
      "source": [
        "# [Optional 10 points] Part 3. Topic model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BwJNkn2DMVT"
      },
      "source": [
        "In this part you will learn how to do topic modeling with common tools and assess the resulting quality of the models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0WcLMhHDMVT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkNFzxqPDMVU"
      },
      "source": [
        "The provided data contain chunked stories by Edgar Allan Poe (EAP), Mary Shelley (MWS), and HP Lovecraft (HPL).\n",
        "\n",
        "The dataset can be downloaded here: `https://drive.google.com/file/d/14tAjAzHr6UmFVFV7ABTyNHBh-dWHAaLH/view?usp=sharing`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BNIa1miDMVU"
      },
      "source": [
        "#### Preprocess dataset with the functions from the Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B8PZNRyDMVV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKU2D29CDMVV"
      },
      "source": [
        "#### Quality estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6xfd3vpDMVW"
      },
      "source": [
        "Implement the following three quality fuctions: `coherence` (or `tf-idf coherence`), `normalized PMI`, `based on the distributed word representation`(you can use pretrained w2v vectors or some other model). You are free to use any libraries (for instance gensim) and components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEtv-gWcDMVW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp1jTJmPDMVX"
      },
      "source": [
        "### Topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgbVtgphDMVX"
      },
      "source": [
        "Read and preprocess the dataset, divide it into train and test parts `sklearn.model_selection.train_test_split`. Test part will be used in classification part. For simplicity we do not perform cross-validation here, but you should remember about it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Uh0J_C8DMVY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNepVz1wDMVY"
      },
      "source": [
        "Plot the histogram of resulting tokens counts in the processed datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_7JVEfbDMVZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulOjNL9vDMVZ"
      },
      "source": [
        "Plot the histogram of resulting tokens counts in the processed datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAse0LBvDMVa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N86AaGNDMVa"
      },
      "source": [
        "#### NMF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPelcAFQDMVa"
      },
      "source": [
        "Implement topic modeling with NMF (you can use `sklearn.decomposition.NMF`) and print out resulting topics. Try to change hyperparameters to better fit the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCPzCMMMDMVa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuaKAGIWDMVb"
      },
      "source": [
        "#### LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAK_MfQxDMVb"
      },
      "source": [
        "Implement topic modeling with LDA (you can use gensim implementation) and print out resulting topics. Try to change hyperparameters to better fit the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdzopuCJDMVb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSF6fq0NDMVc"
      },
      "source": [
        "### Additive regularization of topic models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVcA4itQDMVc"
      },
      "source": [
        "Implement topic modeling with ARTM. You may use bigartm library (simple installation for linux: pip install bigartm) or TopicNet framework (`https://github.com/machine-intelligence-laboratory/TopicNet`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03xhr0PmDMVc"
      },
      "source": [
        "Create artm topic model fit it to the data. Try to change hyperparameters (number of specific and background topics) to better fit the dataset. Play with smoothing and sparsing coefficients (use grid), try to add decorrelator. Print out resulting topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgv1qIUZDMVc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mteOYV8TDMVd"
      },
      "source": [
        "Write a function to convert new documents to topics probabilities vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGkD9YaxDMVd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPrNOdMLDMVd"
      },
      "source": [
        "Calculate the quality scores for each model. Make a barplot to compare the quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQadAQCXDMVd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of HW1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
